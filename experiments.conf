best {
  data_dir = data  # Edit this

  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 5
  top_span_ratio = 0.4
  max_num_extracted_spans = 3900
  max_num_speakers = 20
  max_segment_len = 256

  # Learning
  bert_learning_rate = 1e-5
  task_learning_rate = 2e-4
  loss_type = marginalized  # {marginalized, hinge}
  mention_loss_coef = 0
  false_new_delta = 1.5  # For loss_type = hinge
  adam_eps = 1e-6
  adam_weight_decay = 1e-2
  warmup_ratio = 0.1
  max_grad_norm = 1  # Set 0 to disable clipping
  gradient_accumulation_steps = 1

  # Model hyperparameters.
  coref_depth = 1  # when 1: no higher order (except for cluster_merging)
  higher_order = attended_antecedent # {attended_antecedent, max_antecedent, entity_equalization, span_clustering, cluster_merging}
  coarse_to_fine = true
  fine_grained = true
  dropout_rate = 0.3
  ffnn_size = 1000
  ffnn_depth = 1
  cluster_ffnn_size = 1000   # For cluster_merging
  cluster_reduce = mean  # For cluster_merging
  easy_cluster_first = false  # For cluster_merging
  cluster_dloss = false  # cluster_merging
  num_epochs = 24
  feature_emb_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = true
  use_width_prior = true  # For mention score
  use_distance_prior = true  # For mention-ranking score

  # Other.
  conll_eval_path = ${best.data_dir}/dev.english.v4_gold_conll  # gold_conll file for dev
  conll_test_path = ${best.data_dir}/test.english.v4_gold_conll  # gold_conll file for test
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = ${best.data_dir}

  # new
  amp = True
  hidden_dropout_prob = -1 # less than 0 to disable
}

bert_base = ${best}{
  num_docs = 2802
  bert_learning_rate = 1e-05
  task_learning_rate = 2e-4
  max_segment_len = 128
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 11
  bert_tokenizer_name = bert-base-cased
  bert_pretrained_name_or_path = bert-base-cased
}

train_bert_base = ${bert_base}{
}

spanbert_base = ${best}{
  num_docs = 2802
  bert_learning_rate = 2e-05
  task_learning_rate = 0.0001
  max_segment_len = 384
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 3
  bert_tokenizer_name = bert-base-cased
  bert_pretrained_name_or_path = SpanBERT/spanbert-base-cased
}

debug_spanbert_base = ${spanbert_base}{
}

train_spanbert_base = ${spanbert_base}{
}

spanbert_large = ${best}{
  num_docs = 2802
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0003
  max_segment_len = 512
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 3
  bert_tokenizer_name = bert-base-cased
  bert_pretrained_name_or_path = SpanBERT/spanbert-large-cased
}

spanbert_large_accu_6 = ${spanbert_large}{
  gradient_accumulation_steps = 6
  num_epochs = 144
}

spanbert_large_ours_hp = ${spanbert_large}{
  hidden_dropout_prob = 0.1
  task_learning_rate = 0.01
}

train_spanbert_large = ${spanbert_large}{
}

longformer_base = ${best}{
  num_docs = 2802
  bert_learning_rate = 2e-05
  task_learning_rate = 0.0001
  max_segment_len = 4096
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 1
  bert_tokenizer_name = roberta-base
  bert_pretrained_name_or_path = allenai/longformer-base-4096
  amp = true
}

longformer_base_our_hp = ${longformer_base}{
  hidden_dropout_prob = 0.1
  bert_learning_rate = 1e-5
}

longformer_base_accu_6 = ${longformer_base}{
 gradient_accumulation_steps = 6 
 max_grad_norm = 0
 num_epochs = 144 
}


spanbert_large = ${best}{
  num_docs = 2802
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0003
  max_segment_len = 512
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 3
  bert_tokenizer_name = bert-base-cased
  bert_pretrained_name_or_path = SpanBERT/spanbert-large-cased
}

longformer_large_sbl = ${best}{
  num_docs = 2802
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0003
  max_segment_len = 512
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 3
  bert_tokenizer_name = roberta-large
  bert_pretrained_name_or_path = allenai/longformer-large-4096
  amp = true
}

longformer_large_sbl_cp = ${longformer_large_sbl}{
  bert_tokenizer_name = roberta-base
  max_segment_len = 2048
}


debug_longformer_base = ${longformer_base}{
}

longformer_large = ${best}{
  num_docs = 2802
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0003
  max_segment_len = 2048
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 1
  bert_tokenizer_name = roberta-base
  bert_pretrained_name_or_path = allenai/longformer-large-4096
  amp = true
}

train_longformer_large = ${longformer_large}{
}

train_longformer_large_1e-5_1e-4 = ${longformer_large}{
  task_learning_rate = 0.0001
}

train_longformer_large_1e-5_2e-4 = ${longformer_large}{
  task_learning_rate = 0.0002
}

train_longformer_large_1e-5_3e-4 = ${longformer_large}{
  task_learning_rate = 0.0003
}

train_longformer_large_2e-5_1e-4 = ${longformer_large}{
  bert_learning_rate = 2e-05
  task_learning_rate = 0.0001
}

train_longformer_large_2e-5_2e-4 = ${train_longformer_large_2e-5_1e-4}{
  task_learning_rate = 0.0002
}

train_longformer_large_2e-5_3e-4 = ${train_longformer_large_2e-5_1e-4}{
  task_learning_rate = 0.0003
}

train_longformer_large_3e-5_1e-4 = ${longformer_large}{
  bert_learning_rate = 3e-05
  task_learning_rate = 0.0001
}

train_longformer_large_3e-5_2e-4 = ${train_longformer_large_3e-5_1e-4}{
  task_learning_rate = 0.0002
}

train_longformer_large_3e-5_3e-4 = ${train_longformer_large_3e-5_1e-4}{
  task_learning_rate = 0.0003
}

train_longformer_large_5e-6_1e-4 = ${longformer_large}{
  task_learning_rate = 0.0001
  bert_learning_rate = 5e-06
}

longformer_large_out_hp = ${longformer_large}{
  hidden_dropout_prob = 0.1
  task_learning_rate = 1e-4
}

longformer_large_accu_6 = ${longformer_large}{
  gradient_accumulation_steps = 6
  max_grad_norm = 0
  num_epochs = 144
}
